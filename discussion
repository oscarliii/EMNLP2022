explain CCA results:

It shows that some acoustic information such as prosody is difficult to predict using current SSL scheme which predicts the masked part. It is plausible because such information is variable especially in emotional speech where prosody is highly dependent on dialog context (IEMOCAP is a corpus consisting of acted and improvised emotional dialog). What's more, emotion is majorly represented by salient emotion-related periods instead of the whole speech utterance. Considering these particular facts, unlike phonetic information (language) which is more stable and has a clear speaking pattern (linguistic structure), the masked prosodic information is more difficult to capture using unmasked parts. Also, it also demonstrates that with the layers go deeper, the model gets closer to its original training target -- contrastive predictive loss.

questions:
Hierarchical probing, how to do?

todo:
calculate ACC using hct features.


It is hard to say wav2vec 2.0 (or even most of the self-supervised acoustic models) is the optimal choice for speech emotion recognition and more other downstream tasks. Forget about the performance difference, feature extraction and processing on large corpora using these models are heavy burdens for local PCs, not mention fine-tuning. There is still a long way to go before researchers obtained clear explanation of these models and their representations.